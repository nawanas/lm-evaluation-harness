{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the Blob Service Client.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnawanas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for localhost to your netrc file: C:\\Users\\nawanas\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python -m lm_eval.__main__ --device cpu --log_samples --trust_remote_code --limit 10 --model azure-openai-chat-completions --apply_chat_template --model_args model=gpt-4o-uks,base_url=https://aoai-aiq-02-uk-south.openai.azure.com/openai/deployments/gpt-4ouks/chat/completions?api-version=2024-02-15-preview --include_path lm_eval/tasks/kobest --tasks kobest_hellaswag_direct --output_path C:/Users/nawanas/source/repos/lm-evaluation-harness-2/kobest_hellaswag_direct/gpt-4o\n",
      "Output: WARNING 12-04 16:06:06 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 12-04 16:06:06 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n",
      "azure-openai-chat-completions (model=gpt-4o-uks,base_url=https://aoai-aiq-02-uk-south.openai.azure.com/openai/deployments/gpt-4ouks/chat/completions?api-version=2024-02-15-preview,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: 1\n",
      "|         Tasks         |Version|Filter|n-shot|Total|Effective|  Metric   |   |Value|   |Stderr|\n",
      "|-----------------------|------:|------|-----:|----:|--------:|-----------|---|----:|---|-----:|\n",
      "|kobest_hellaswag_direct|      2|none  |     0|  500|       10|exact_match|â†‘  |  0.7|Â±  |0.1528|\n",
      "\n",
      "\n",
      "Errors: 2024-12-04 16:05:50.602098: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 16:05:55.399623: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04:16:06:08,475 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-12-04:16:06:08,475 INFO     [__main__.py:307] Including path: lm_eval/tasks/kobest\n",
      "2024-12-04:16:06:56,964 WARNING  [__main__.py:316]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-12-04:16:06:56,968 INFO     [__main__.py:368] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
      "2024-12-04:16:06:56,968 INFO     [__main__.py:380] Selected Tasks: ['kobest_hellaswag_direct']\n",
      "2024-12-04:16:06:57,114 INFO     [evaluator.py:166] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-12-04:16:06:57,114 INFO     [evaluator.py:203] Initializing azure-openai-chat-completions model, with arguments: {'model': 'gpt-4o-uks', 'base_url': 'https://aoai-aiq-02-uk-south.openai.azure.com/openai/deployments/gpt-4ouks/chat/completions?api-version=2024-02-15-preview', 'trust_remote_code': True}\n",
      "2024-12-04:16:06:57,114 WARNING  [openai_completions.py:106] chat-completions endpoint requires the `--apply_chat_template` flag.\n",
      "2024-12-04:16:06:57,114 INFO     [api_models.py:114] Using max length 2048 - 1\n",
      "2024-12-04:16:06:57,114 INFO     [api_models.py:117] Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\n",
      "2024-12-04:16:06:57,115 INFO     [api_models.py:128] Using tokenizer tiktoken\n",
      "2024-12-04:16:07:00,473 WARNING  [task.py:325] [Task: kobest_hellaswag_direct] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-12-04:16:07:00,474 WARNING  [task.py:325] [Task: kobest_hellaswag_direct] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-12-04:16:07:00,513 WARNING  [evaluator.py:408] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2024-12-04:16:07:00,513 INFO     [task.py:415] Building contexts for kobest_hellaswag_direct on rank 0...\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1092.61it/s]\n",
      "2024-12-04:16:07:00,525 INFO     [evaluator.py:498] Running generate_until requests\n",
      "\n",
      "Requesting API:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Requesting API:  10%|â–ˆ         | 1/10 [00:04<00:43,  4.82s/it]\n",
      "Requesting API:  20%|â–ˆâ–ˆ        | 2/10 [00:09<00:38,  4.81s/it]\n",
      "Requesting API:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:14<00:33,  4.84s/it]\n",
      "Requesting API:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:19<00:29,  4.86s/it]\n",
      "Requesting API:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:24<00:24,  4.96s/it]\n",
      "Requesting API:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:29<00:19,  4.90s/it]\n",
      "Requesting API:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:34<00:14,  4.87s/it]\n",
      "Requesting API:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:38<00:09,  4.86s/it]\n",
      "Requesting API:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:43<00:04,  4.84s/it]\n",
      "Requesting API: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/it]\n",
      "Requesting API: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/it]\n",
      "2024-12-04:16:07:53,723 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2024-12-04:16:07:53,725 INFO     [evaluation_tracker.py:287] Saving per-sample results for: kobest_hellaswag_direct\n",
      "\n",
      "Blob path: kobest_hellaswag_direct\\gpt-4o-uks/\n",
      "Full path: C:/Users/nawanas/source/repos/lm-evaluation-harness-2/kobest_hellaswag_direct/gpt-4o\\gpt-4o-uks\n",
      "Uploaded results_2024-11-27T12-27-04.197955.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T12-27-04.197955.json\n",
      "Uploaded results_2024-11-27T13-28-15.508822.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T13-28-15.508822.json\n",
      "Uploaded results_2024-11-27T13-37-39.535657.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T13-37-39.535657.json\n",
      "Uploaded results_2024-11-27T13-42-29.284021.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T13-42-29.284021.json\n",
      "Uploaded results_2024-11-27T13-58-55.072286.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T13-58-55.072286.json\n",
      "Uploaded results_2024-11-27T14-02-10.145452.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T14-02-10.145452.json\n",
      "Uploaded results_2024-11-27T14-08-50.125714.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T14-08-50.125714.json\n",
      "Uploaded results_2024-11-27T15-02-02.220327.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-02-02.220327.json\n",
      "Uploaded results_2024-11-27T15-08-46.959175.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-08-46.959175.json\n",
      "Uploaded results_2024-11-27T15-13-29.972455.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-13-29.972455.json\n",
      "Uploaded results_2024-11-27T15-16-40.950850.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-16-40.950850.json\n",
      "Uploaded results_2024-11-27T15-22-43.347114.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-22-43.347114.json\n",
      "Uploaded results_2024-11-27T15-29-31.517561.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-29-31.517561.json\n",
      "Uploaded results_2024-11-27T15-38-13.544250.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-11-27T15-38-13.544250.json\n",
      "Uploaded results_2024-12-03T14-06-55.361153.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-12-03T14-06-55.361153.json\n",
      "Uploaded results_2024-12-04T08-34-22.905661.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-12-04T08-34-22.905661.json\n",
      "Uploaded results_2024-12-04T15-56-09.881235.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-12-04T15-56-09.881235.json\n",
      "Uploaded results_2024-12-04T16-07-53.724106.json to kobest_hellaswag_direct\\gpt-4o-uks/results_2024-12-04T16-07-53.724106.json\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T12-27-04.197955.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T12-27-04.197955.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-28-15.508822.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T13-28-15.508822.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-37-39.535657.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T13-37-39.535657.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-42-29.284021.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T13-42-29.284021.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-58-55.072286.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T13-58-55.072286.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T14-02-10.145452.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T14-02-10.145452.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T14-08-50.125714.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T14-08-50.125714.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-02-02.220327.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-02-02.220327.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-08-46.959175.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-08-46.959175.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-13-29.972455.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-13-29.972455.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-16-40.950850.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-16-40.950850.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-22-43.347114.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-22-43.347114.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-29-31.517561.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-29-31.517561.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-38-13.544250.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-11-27T15-38-13.544250.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-03T14-06-55.361153.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-12-03T14-06-55.361153.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-04T08-34-22.905661.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-12-04T08-34-22.905661.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-04T15-56-09.881235.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-12-04T15-56-09.881235.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-04T16-07-53.724106.jsonl to kobest_hellaswag_direct\\gpt-4o-uks/samples_kobest_hellaswag_direct_2024-12-04T16-07-53.724106.jsonl\n",
      "Running command: python -m lm_eval.__main__ --device cpu --log_samples --trust_remote_code --limit 10 --model azure-openai-chat-completions --apply_chat_template --model_args model=gpt-4o-mini,base_url=https://aoai-aiq-02-uk-south.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview --include_path lm_eval/tasks/kobest --tasks kobest_hellaswag_direct --output_path C:/Users/nawanas/source/repos/lm-evaluation-harness-2/kobest_hellaswag_direct/gpt-4o-mini\n",
      "Output: WARNING 12-04 16:08:12 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 12-04 16:08:12 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n",
      "azure-openai-chat-completions (model=gpt-4o-mini,base_url=https://aoai-aiq-02-uk-south.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: 1\n",
      "|         Tasks         |Version|Filter|n-shot|Total|Effective|  Metric   |   |Value|   |Stderr|\n",
      "|-----------------------|------:|------|-----:|----:|--------:|-----------|---|----:|---|-----:|\n",
      "|kobest_hellaswag_direct|      2|none  |     0|  500|       10|exact_match|â†‘  |  0.6|Â±  |0.1633|\n",
      "\n",
      "\n",
      "Errors: 2024-12-04 16:08:04.477855: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 16:08:06.140786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04:16:08:14,150 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-12-04:16:08:14,150 INFO     [__main__.py:307] Including path: lm_eval/tasks/kobest\n",
      "2024-12-04:16:08:22,629 WARNING  [__main__.py:316]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-12-04:16:08:22,632 INFO     [__main__.py:368] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
      "2024-12-04:16:08:22,632 INFO     [__main__.py:380] Selected Tasks: ['kobest_hellaswag_direct']\n",
      "2024-12-04:16:08:22,640 INFO     [evaluator.py:166] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-12-04:16:08:22,640 INFO     [evaluator.py:203] Initializing azure-openai-chat-completions model, with arguments: {'model': 'gpt-4o-mini', 'base_url': 'https://aoai-aiq-02-uk-south.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview', 'trust_remote_code': True}\n",
      "2024-12-04:16:08:22,640 WARNING  [openai_completions.py:106] chat-completions endpoint requires the `--apply_chat_template` flag.\n",
      "2024-12-04:16:08:22,640 INFO     [api_models.py:114] Using max length 2048 - 1\n",
      "2024-12-04:16:08:22,640 INFO     [api_models.py:117] Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\n",
      "2024-12-04:16:08:22,640 INFO     [api_models.py:128] Using tokenizer tiktoken\n",
      "2024-12-04:16:08:25,676 WARNING  [task.py:325] [Task: kobest_hellaswag_direct] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-12-04:16:08:25,676 WARNING  [task.py:325] [Task: kobest_hellaswag_direct] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-12-04:16:08:25,702 WARNING  [evaluator.py:408] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2024-12-04:16:08:25,703 INFO     [task.py:415] Building contexts for kobest_hellaswag_direct on rank 0...\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1461.28it/s]\n",
      "2024-12-04:16:08:25,712 INFO     [evaluator.py:498] Running generate_until requests\n",
      "\n",
      "Requesting API:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Requesting API:  10%|â–ˆ         | 1/10 [00:04<00:43,  4.88s/it]\n",
      "Requesting API:  20%|â–ˆâ–ˆ        | 2/10 [00:09<00:38,  4.82s/it]\n",
      "Requesting API:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:14<00:33,  4.80s/it]\n",
      "Requesting API:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:19<00:29,  4.84s/it]\n",
      "Requesting API:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:24<00:24,  4.81s/it]\n",
      "Requesting API:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:28<00:19,  4.79s/it]\n",
      "Requesting API:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:33<00:14,  4.78s/it]\n",
      "Requesting API:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:38<00:09,  4.84s/it]\n",
      "Requesting API:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:43<00:04,  4.98s/it]\n",
      "Requesting API: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.94s/it]\n",
      "Requesting API: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/it]\n",
      "2024-12-04:16:09:15,974 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2024-12-04:16:09:15,975 INFO     [evaluation_tracker.py:287] Saving per-sample results for: kobest_hellaswag_direct\n",
      "\n",
      "Blob path: kobest_hellaswag_direct\\gpt-4o-mini/\n",
      "Full path: C:/Users/nawanas/source/repos/lm-evaluation-harness-2/kobest_hellaswag_direct/gpt-4o-mini\\gpt-4o-mini\n",
      "Uploaded results_2024-11-27T13-29-33.563099.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T13-29-33.563099.json\n",
      "Uploaded results_2024-11-27T13-38-57.447643.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T13-38-57.447643.json\n",
      "Uploaded results_2024-11-27T13-43-53.552695.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T13-43-53.552695.json\n",
      "Uploaded results_2024-11-27T14-00-09.449286.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T14-00-09.449286.json\n",
      "Uploaded results_2024-11-27T14-03-28.083771.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T14-03-28.083771.json\n",
      "Uploaded results_2024-11-27T14-10-03.309491.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T14-10-03.309491.json\n",
      "Uploaded results_2024-11-27T15-03-20.817593.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-03-20.817593.json\n",
      "Uploaded results_2024-11-27T15-10-01.222148.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-10-01.222148.json\n",
      "Uploaded results_2024-11-27T15-14-50.005950.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-14-50.005950.json\n",
      "Uploaded results_2024-11-27T15-18-07.281806.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-18-07.281806.json\n",
      "Uploaded results_2024-11-27T15-24-03.221689.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-24-03.221689.json\n",
      "Uploaded results_2024-11-27T15-30-46.184637.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-30-46.184637.json\n",
      "Uploaded results_2024-11-27T15-40-52.492781.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-11-27T15-40-52.492781.json\n",
      "Uploaded results_2024-12-03T14-08-19.308354.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-12-03T14-08-19.308354.json\n",
      "Uploaded results_2024-12-04T08-35-43.119853.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-12-04T08-35-43.119853.json\n",
      "Uploaded results_2024-12-04T16-09-15.975189.json to kobest_hellaswag_direct\\gpt-4o-mini/results_2024-12-04T16-09-15.975189.json\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-29-33.563099.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T13-29-33.563099.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-38-57.447643.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T13-38-57.447643.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T13-43-53.552695.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T13-43-53.552695.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T14-00-09.449286.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T14-00-09.449286.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T14-03-28.083771.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T14-03-28.083771.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T14-10-03.309491.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T14-10-03.309491.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-03-20.817593.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-03-20.817593.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-10-01.222148.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-10-01.222148.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-14-50.005950.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-14-50.005950.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-18-07.281806.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-18-07.281806.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-24-03.221689.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-24-03.221689.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-30-46.184637.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-30-46.184637.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-11-27T15-40-52.492781.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-11-27T15-40-52.492781.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-03T14-08-19.308354.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-12-03T14-08-19.308354.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-04T08-35-43.119853.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-12-04T08-35-43.119853.jsonl\n",
      "Uploaded samples_kobest_hellaswag_direct_2024-12-04T16-09-15.975189.jsonl to kobest_hellaswag_direct\\gpt-4o-mini/samples_kobest_hellaswag_direct_2024-12-04T16-09-15.975189.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nawanas\\source\\repos\\custom-model-program\\model-evaluation\\external\\lm-evaluation-harness\\wandb\\run-20241204_160919-qmlswqq2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/qmlswqq2' target=\"_blank\">kobest_hellaswag_direct</a></strong> to <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct' target=\"_blank\">http://localhost:8080/nawanas/kobest_hellaswag_direct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/qmlswqq2' target=\"_blank\">http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/qmlswqq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error listing blobs: The requested URI does not represent any resource on the server.\n",
      "RequestId:dd9da16d-501e-004f-3ba9-469784000000\n",
      "Time:2024-12-05T00:09:22.0771516Z\n",
      "ErrorCode:InvalidUri\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<Error><Code>InvalidUri</Code><Message>The requested URI does not represent any resource on the server.\n",
      "RequestId:dd9da16d-501e-004f-3ba9-469784000000\n",
      "Time:2024-12-05T00:09:22.0771516Z</Message></Error>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qmlswqq2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kobest_hellaswag_direct</strong> at: <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/qmlswqq2' target=\"_blank\">http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/qmlswqq2</a><br/> View project at: <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct' target=\"_blank\">http://localhost:8080/nawanas/kobest_hellaswag_direct</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241204_160919-qmlswqq2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qmlswqq2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nawanas\\source\\repos\\custom-model-program\\model-evaluation\\external\\lm-evaluation-harness\\wandb\\run-20241204_160920-6xhsuk42</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/6xhsuk42' target=\"_blank\">kobest_hellaswag_direct</a></strong> to <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct' target=\"_blank\">http://localhost:8080/nawanas/kobest_hellaswag_direct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/6xhsuk42' target=\"_blank\">http://localhost:8080/nawanas/kobest_hellaswag_direct/runs/6xhsuk42</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error listing blobs: The requested URI does not represent any resource on the server.\n",
      "RequestId:dd9da3a3-501e-004f-78a9-469784000000\n",
      "Time:2024-12-05T00:09:24.2479197Z\n",
      "ErrorCode:InvalidUri\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<Error><Code>InvalidUri</Code><Message>The requested URI does not represent any resource on the server.\n",
      "RequestId:dd9da3a3-501e-004f-78a9-469784000000\n",
      "Time:2024-12-05T00:09:24.2479197Z</Message></Error>\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import wandb\n",
    "import os\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import time\n",
    "\n",
    "# Place the blob storage informaiton to access storing and reading files\n",
    "account_url = '<ACCOUNT_URL>'\n",
    "sas_token = '<SAS_TOKEN>'\n",
    "container_name = '<CONTAINER_NAME>'\n",
    "      \n",
    "try:\n",
    "    # Create the BlobServiceClient object using the account URL and SAS token\n",
    "    blob_service_client = BlobServiceClient(account_url=account_url, credential=sas_token)\n",
    "    print(\"Successfully connected to the Blob Service Client.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to the Blob Service Client: {e}\")\n",
    "\n",
    "\n",
    "def run_wandb(output_file_path, container_name, project_name, _run_name, dataset, metric_used,filter_used,sample_main_segment,options_dict):\n",
    "# This function takes the following elements\n",
    "# output_file_path: This is the directory in the blob store that contains the output results and log_files\n",
    "# container_name: Name of the container that contains the output files\n",
    "# project_name: This is the name of the project in WandB under the account defined by the WandB_API_KEY\n",
    "# _run_name: This is the name of the run that is defined as a grouping criteria\n",
    "# dataset: This is the individual segments of interest in the results, they would be the name of the groups that are included in the YAML file\n",
    "# metric_used: This is the metric used to assess the performance of the model on this dataset\n",
    "# filer_used: the type of filter used on the output of the evaluation \n",
    "# sample_main_segment: the label of the main input in the data set\n",
    "# options_dict: the dictionary containing the labels of the options/multiple choice answers in the data\n",
    "\n",
    "    # initalize the WandB run within the sepefic project and grouped by the run name\n",
    "    wandb.init(project=project_name,name='_'.join(dataset),group=_run_name)\n",
    "    \n",
    "    # if not sas_token or not account_url:\n",
    "    #     raise ValueError(\"AZURE_STORAGE_SAS_TOKEN or AZURE_ACCOUNT_URL environment variable is not set\")\n",
    "    \n",
    "    # # Use the SAS token to authenticate\n",
    "    # blob_service_client = BlobServiceClient(account_url=account_url, credential=sas_token)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # # Use DefaultAzureCredential to authenticate with managed identity\n",
    "    # credential = DefaultAzureCredential()\n",
    "\n",
    "    # # Read account_url from environment variable\n",
    "    # account_url = os.getenv('AZURE_ACCOUNT_URL')\n",
    "    # if not account_url:\n",
    "    #     raise ValueError(\"AZURE_ACCOUNT_URL environment variable is not set\")\n",
    "    \n",
    "    # blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\n",
    "    # container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "\n",
    "    try:\n",
    "        # List blobs that contain _run_name in their name\n",
    "        blob_list = container_client.list_blobs()\n",
    "        \n",
    "        # Debug: Print all blob names\n",
    "        all_blobs = [blob.name for blob in blob_list]\n",
    "        print(\"All blobs:\", all_blobs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing blobs: {e}\")\n",
    "        return\n",
    "\n",
    "    folder_path = f\"{output_file_path}\"\n",
    "    \n",
    "    # Filter blobs to include only those in the folder labeled with _run_name\n",
    "    json_files = [blob for blob in all_blobs if blob.startswith(folder_path) and blob.endswith('.json')]\n",
    "    jsonl_files = [blob for blob in all_blobs if blob.startswith(folder_path) and blob.endswith('.jsonl')]\n",
    "\n",
    "    # Debug: Print filtered blob names\n",
    "    print(\"Filtered JSON files:\", json_files)\n",
    "    print(\"Filtered JSONL files:\", jsonl_files)\n",
    "\n",
    "    # Download the filtered blobs\n",
    "    for blob_name in json_files:\n",
    "        download_file_path = os.path.join(output_file_path, os.path.basename(blob_name))\n",
    "        os.makedirs(os.path.dirname(download_file_path), exist_ok=True)\n",
    "        \n",
    "        with open(download_file_path, \"wb\") as download_file:\n",
    "            output_data = container_client.download_blob(blob_name)\n",
    "            download_file.write(output_data.readall())\n",
    "            print(f\"Downloaded {blob_name} to {download_file_path}\")\n",
    "\n",
    "    # # Read the output file\n",
    "    # for file_name in json_files:\n",
    "    #     blob_client = container_client.get_blob_client(file_name)\n",
    "    #     blob_data = blob_client.download_blob().readall()\n",
    "    #     output_data = json.loads(blob_data)\n",
    "        \n",
    "        # Extract accuracy metrics based on the provided datasets and metric\n",
    "        accuracy_data = []\n",
    "        results = output_data.get('results', {})\n",
    "\n",
    "        \n",
    "        for dataset_name, metrics in results.items():\n",
    "            if dataset_name in dataset and (metric_used +',' + filter_used) in metrics:\n",
    "                accuracy_data.append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'metric': metrics[(metric_used +',' + filter_used)],\n",
    "                    'stderr': metrics[(metric_used +'_stderr,' + filter_used)]\n",
    "                })\n",
    "\n",
    "        \n",
    "        # Log the accuracy and matching/unmatching results to wandb\n",
    "        for data in accuracy_data:\n",
    "            wandb.log({\n",
    "                str(metric_used): data['metric'],\n",
    "                str(metric_used + \"_stderr\"): data['stderr']\n",
    "            })\n",
    "\n",
    "\n",
    "    # Filter JSONL files based on group_subtasks\n",
    "    for dataset_name in dataset:\n",
    "        matching_jsonl_files = [jsonl_file for jsonl_file in jsonl_files]\n",
    "        for jsonl_file in matching_jsonl_files:\n",
    "            df = process_jsonl_file_from_blob(container_client, jsonl_file,_run_name + '_' + \"_\".join(dataset),metric_used,sample_main_segment,options_dict)\n",
    "            wandb.log({\n",
    "                \"table\": wandb.Table(dataframe=df)\n",
    "            })\n",
    "\n",
    "\n",
    "def process_jsonl_file_from_blob(container_client, jsonl_file_path,dataset_name,metric_used,sample_main_segment,options_dict):\n",
    "# extracts table from the log sample file\n",
    "# reads the jsonl file from the container defined by the jsonl_file_path and the container_client\n",
    "# identifies the segments of interest based on the dataset_name \n",
    "# metric_used: defines the metric that is used in the assessment of the performance\n",
    "# sample_main_segment: the label of the main input in the data set\n",
    "# options_dict: the dictionary containing the labels of the options/multiple choice answers in the data\n",
    "\n",
    "    records = []\n",
    "    \n",
    "    blob_client = container_client.get_blob_client(jsonl_file_path)\n",
    "    blob_data = blob_client.download_blob().readall().decode('utf-8')\n",
    "    \n",
    "    for line in blob_data.splitlines():\n",
    "        record = json.loads(line)\n",
    "        data_id = record.get('doc_id')\n",
    "        context = record.get('doc', {}).get(sample_main_segment)\n",
    "        target = record.get('target')\n",
    "        resps = record.get('resps', [])[0][0] if record.get('resps') else None\n",
    "        filtered_resps = record.get('filtered_resps',[])[0] if record.get('filtered_resps') else None\n",
    "        result = record.get(metric_used)\n",
    "    \n",
    "        # Extract options and choices\n",
    "        endings = {key: record['doc'].get(key, \"\") for key in options_dict}\n",
    "        choices = {key: value for key, value in endings.items() if value}\n",
    "\n",
    "        context_with_choices = context  + \"\\n\" + \"\\n\".join([f\"{key}: {value}\" for key, value in choices.items()])\n",
    "        \n",
    "        records.append({\n",
    "            'dataset': dataset_name,\n",
    "            'result' : result,\n",
    "            'data_id': data_id,\n",
    "            'question': context_with_choices,\n",
    "            'target': target,\n",
    "            'response': resps,\n",
    "            'filtered_response': filtered_resps\n",
    "        })\t\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "base_command = [\n",
    "    \"python\", \"-m\", \"lm-evaluation-harness/lm_eval.__main__\",\n",
    "    \"--device\", \"cpu\",\n",
    "    \"--log_samples\",\n",
    "    \"--trust_remote_code\",\n",
    "    \"--limit\",\"10\"\n",
    "]\n",
    "\n",
    "output_file_base_path = '<BASE_PATH>'\n",
    "output_file_full_path = \"<FULL_PATH>\"\n",
    "task = '<TASK_NAME>'\n",
    "include_path = '<TASK_PATH>'\n",
    "#The label on the segment in the data that represents the input to the model e.g. question/context/segment etc...\n",
    "sample_main_segment = '<Segment_Label>'\n",
    "#Listing of ways the options are labeled in the testing data, e.g. 1,2,3,4; A,B,C,D; etc.\n",
    "options_dict = ['<Label_0>','<label_1>','<LABEL_2>','<LABEL_3>']\n",
    "\n",
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [\n",
    "            {\n",
    "            # Model name such as azure_open_chat_completions or hf or phi\n",
    "            \"name\": \"<model_name>\",\n",
    "            #Template to use if using chat endpoint, otherwise \"\"\n",
    "            \"Template\": \"--apply_chat_template\",\n",
    "            # Run name to use in the output file\n",
    "            \"run_name\": \"<run_name>\",\n",
    "            # Path to the output directory\n",
    "            \"path\": \"<path>\",\n",
    "            # Model arguments for the harness, this can include the model deployment name and URL \n",
    "            \"args\": \"model=<model>,base_url=<base_url>\",\n",
    "            # If you need to pass extra argument to the lm_eval call\n",
    "            \"extra_args\": \"\",\n",
    "            # The list of groups/datasets to evaluate from the YAML file\n",
    "            \"Datasets\": ['<daaset_name>'],\n",
    "            # Metric to use, e.g. exact match \n",
    "            \"metric\": '<metric>',\n",
    "            # Filter deployed or none if no filter is used \n",
    "            'filter': '<Filer>',\n",
    "            #You need to setup the API key foe the model as an env variable\n",
    "            'API_KEY': '<Name of API key env varibale>',\n",
    "            # set the sleep time of the call if needed, 0 if not needed\n",
    "            'SLEEP': 4\n",
    "        },\n",
    "    ]\n",
    "\n",
    "wandb.login(host='http://localhost:8080',key= os.getenv('WANDB_API_KEY'))\n",
    "\n",
    "# Run the evaluation for each model\n",
    "for model in models:\n",
    "    filepath = output_file_full_path + \"/\" + output_file_base_path + \"/\" + model[\"path\"]\n",
    "    command = base_command + [\"--model\", model[\"name\"], model['Template'], \"--model_args\", model[\"args\"], \"--include_path\", include_path, \"--tasks\", task, \"--output_path\", filepath]\n",
    "\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env['API_KEY'] = str(os.getenv(model['API_KEY']))\n",
    "    env['API_SLEEP'] = str(model['SLEEP'])\n",
    "\n",
    "    # Print the command to verify it\n",
    "    print(\"Running command:\", \" \".join(command))\n",
    "\n",
    "    # Run the command and capture output and errors\n",
    "    result = subprocess.run(command, env=env, capture_output=True, text=True)\n",
    "\n",
    "    # Print the output and errors for debugging\n",
    "    print(\"Output:\", result.stdout)\n",
    "    print(\"Errors:\", result.stderr)\n",
    "\n",
    "    # Check if the command was successful\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Command failed with return code {result.returncode}\")\n",
    "    else:\n",
    "        # Upload files to the blob store\n",
    "        blob_path = os.path.join(output_file_base_path, model['run_name']) + \"/\"\n",
    "\n",
    "        listing_file = os.path.join(filepath, model['run_name'])\n",
    "\n",
    "        print(\"Blob path:\", blob_path)\n",
    "        print(\"Full path:\", listing_file)\n",
    "\n",
    "        try:\n",
    "            files = os.listdir(listing_file)\n",
    "            for file in files:\n",
    "                file_path = os.path.join(listing_file, file)\n",
    "                blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path + file)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as data:\n",
    "                        blob_client.upload_blob(data, overwrite=True)\n",
    "                        print(f\"Uploaded {file} to {blob_path + file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to upload {file} to {blob_path + file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to list files in the directory {listing_file}: {e}\")\n",
    "                \n",
    "for model in models:\n",
    "    for dataset in model[\"Datasets\"]:\n",
    "        filepath = container_name + \"/\" + output_file_base_path + \"/\" + model['run_name']\n",
    "        # Initialize a new wandb run\n",
    "        run_wandb(filepath, container_name, task, model[\"run_name\"], [dataset],model['metric'],model['filter'],sample_main_segment,options_dict)\n",
    "\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
